---
title: "Extract ANEEL's Tariff pre-2010"
author: "Beatriz Couto Ribeiro"
date: "2024-09-18"
output: html_document
editor_options: 
  chunk_output_type: console
---


# Load Packages
```{r setup, include=FALSE}

rm(list=ls())


if (!require("pacman")) install.packages("pacman") #pacman will not accept a character vector so the same packages are repeated

pacman::p_load(tidyverse, #packages for data science
               plm, #estimation of linear panel models
               ggplot2,  #creating graphics
               devtools, #web developer tools 
               rmarkdown, #reproducibility
               tidyr,  #changing the shape and hierarchy of a data set
               dplyr, #grammar of data manipulation
               Synth, #importing and exporting
               SCtools, #extensions for Synthetic Controls Analysis
               panelView, #visualize data panels
               httr, # call url
               jsonlite, # use API
               ggrepel, #labels with ggplot
               ggthemes, #different graph themes for ggplot
               ggpubr, #put figures together
               rvest,
               htmltools,
               readtext,
               readr,
               pdftools,
               stringr,
               data.table, 
               tabulapdf,
               stringdist,
               tesseract,
               magick,
               rJava) # Fast aggregation of large data

# Youtube Video to Change Java's version and install the package: https://www.youtube.com/watch?v=nlsWjezvsg8&t=428s
# library(tabulizer)
# 
# remotes::install_github(c("ropensci/tabulizerjars", "ropensci/tabulizer"), INSTALL_opts = "--no-multiarch")
# 
# remotes::install_github(c("ropensci/tabulapdf"))


```

#Data
```{r}

getwd()

setwd("C:/Users/Ribeiro/Documents/POS-GRADUAÇÃO/POSTDOC/ANEEL_tariff")

```

#Load file to retrieve documents 
```{r}

# Define the path to your CSV file
source_csv <- read.csv("file_ANEEL_Links_documents_scrap.csv")

```

#Download PDF files
```{r}

# Define the path to your CSV file
csv_file_path <- "C:/Users/Ribeiro/Documents/POS-GRADUAÇÃO/POSTDOC/ANEEL_tariff/file_ANEEL_Links_documents_scrap.csv"

# Define the directory where you want to save the downloaded PDFs
download_directory <- "C:/Users/Ribeiro/Documents/POS-GRADUAÇÃO/POSTDOC/ANEEL_tariff/ANEEL_tariff_documents"


# Create the download directory if it doesn't exist
if (!dir.exists(download_directory)) {
  dir.create(download_directory)
}

# Read the CSV file
data <- read_csv(csv_file_path)

# Ensure the necessary columns exist
required_columns <- c("Link", "Sigla", "Ano")
missing_columns <- setdiff(required_columns, colnames(data))
if (length(missing_columns) > 0) {
  stop("The CSV file is missing the following columns: ", paste(missing_columns, collapse = ", "))
}

# Function to download a single PDF
download_pdf <- function(url, destfile) {
  tryCatch({
    GET(url, write_disk(destfile, overwrite = TRUE))
    message("Downloaded: ", url)
  }, error = function(e) {
    message("Failed to download: ", url, " - ", e$message)
  })
}

# Loop through each Link and download the PDF
for (i in 1:nrow(data)) {
  url <- data$Link[i]
  sigla <- data$Sigla[i]
  ano <- data$Ano[i]
  # Construct the file name using Sigla and Ano
  file_name <- paste0(sigla, "_", ano, ".pdf")
  # Define the destination file path
  destfile <- file.path(download_directory, file_name)
  # Download the PDF
  download_pdf(url, destfile)
}

```

# Retrieve ALL INFORMATION (residential, rural....) the role table from PDF
```{r}


# Define folder paths
input_folder <- "C:/Users/Ribeiro/Documents/POS-GRADUAÇÃO/POSTDOC/ANEEL_tariff/ANEEL_tariff_documents"
output_folder <- "C:/Users/Ribeiro/Documents/POS-GRADUAÇÃO/POSTDOC/ANEEL_tariff/ANEEL_tariff_all"


# Ensure output folder exists
if (!dir.exists(output_folder)) {
  dir.create(output_folder, recursive = TRUE)
}

# List all PDF files in the input folder
pdf_files <- list.files(input_folder, pattern = "\\.pdf$", full.names = TRUE)

# Define the relevant column patterns to search for (with relaxed matching for "DEMAN" and "CONSU")
columns_to_extract <- c("TARIFA CONVENCIONAL", "DEMAN", "CONSU")
                        # , 
                        # "A2", "A3", "A3 COOPERATIVA DE ELETRIFICAÇÃO RURAL", 
                        # "A3a", "A4", "A4a COOPERATIVA DE ELETRIFICAÇÃO RURAL TIPO 1", 
                        # "A4b COOPERATIVA DE ELETRIFICAÇÃO RURAL TIPO 2", 
                        # "A4c COOPERATIVA DE ELETRIFICAÇÃO RURAL TIPO 3", 
                        # "AS (Subterrâneo)", "B1-RESIDENCIAL", 
                        # "B1-RESIDENCIAL BAIXA RENDA", "B2-RURAL", 
                        # "B2-COOPERATIVA DE ELETRIFICAÇÃO RURAL", 
                        # "B2-SERVIÇO DE IRRIGAÇÃO")

# Function to extract only the first table from a PDF file and save it as CSV
extract_first_table_and_save <- function(pdf_file, output_folder, columns_to_extract) {
  # Extract all tables from the PDF
  tables <- extract_tables(pdf_file, guess = TRUE)
  
  # Check if any tables were extracted
  if (length(tables) > 0) {
    # Extract the first table only
    first_table <- tables[[1]]
    
    # Convert the table to a data frame
    df <- as.data.frame(first_table, stringsAsFactors = FALSE)
    
    # Filter the data frame for relevant columns using relaxed pattern matching
    selected_columns <- df[, grepl(paste(columns_to_extract, collapse = "|"), df, ignore.case = TRUE), drop = FALSE]
    
    # Check if the resulting data frame has any relevant columns
    if (ncol(selected_columns) > 0) {
      # Create an output file name
      output_file <- paste0(output_folder, "/", tools::file_path_sans_ext(basename(pdf_file)), "_first_table.csv")
      
      # Save the filtered table to a CSV file with UTF-8 encoding for Portuguese
      write.csv(selected_columns, output_file, row.names = FALSE, fileEncoding = "UTF-8")
      cat("Saved first table to:", output_file, "\n")
    } else {
      cat("No relevant columns found in", pdf_file, "\n")
    }
  } else {
    cat("No tables found in", pdf_file, "\n")
  }
}

# Loop over all PDF files and apply the extraction function
for (pdf_file in pdf_files) {
  extract_first_table_and_save(pdf_file, output_folder, columns_to_extract)
}

```


# Retrieve only the variable "B1 - Residencial" - Final
```{r}

input_folder <- "C:/Users/Ribeiro/Documents/POS-GRADUAÇÃO/POSTDOC/ANEEL_tariff/ANEEL_tariff_documents"
output_folder <- "C:/Users/Ribeiro/Documents/POS-GRADUAÇÃO/POSTDOC/ANEEL_tariff/ANEEL_tariff_residential_csv"

# Ensure output folder exists
if (!dir.exists(output_folder)) {
  dir.create(output_folder, recursive = TRUE)
}

# List all PDF files in the input folder
pdf_files <- list.files(input_folder, pattern = "\\.pdf$", full.names = TRUE)

# Function to extract the row with "B1-RESIDENCIAL" (allowing up to 1 character difference)
# and retrieve the full number after it from all tables in the PDF
extract_first_valid_row_and_save <- function(pdf_file, output_folder) {
  # Extract the filename without extension
  file_name <- tools::file_path_sans_ext(basename(pdf_file))
  
  # Split the file name into ID and year
  file_parts <- unlist(strsplit(file_name, "_"))
  ID <- file_parts[1]
  year <- file_parts[2]
  
  # Extract all tables from the PDF
  tables <- extract_tables(pdf_file, guess = TRUE)
  
  # Define the standard column names for the output
  standard_columns <- c("ID", "year", "B1_RESIDENCIAL", "role_number", "table_index")
  
  # Loop over all extracted tables
  for (table_index in seq_along(tables)) {
    table <- tables[[table_index]]
    
    # Convert the table to a data frame
    df <- as.data.frame(table, stringsAsFactors = FALSE)
    
    # Fuzzy match for "B1-RESIDENCIAL" (allowing up to 1 character difference)
    target_row_index <- which(stringdist::stringdist(df[, 1], "B1-RESIDENCIAL", method = "lv") <= 1)
    
    # Check if there is a match
    if (length(target_row_index) > 0) {
      # Extract the matching row
      selected_row <- df[target_row_index, ]
      
      # Retrieve the full number(s) in the row after "B1-RESIDENCIAL"
      row_values <- unlist(regmatches(selected_row[1, ], gregexpr("\\d+(\\.\\d+)?", selected_row[1, ])))
      
      # Remove any erroneous "1" values that might be captured
      row_values <- row_values[row_values != "1"]
      
      # Limit the extracted numbers to the first 2
      row_values <- head(row_values, 2)
      
      # Check if we successfully retrieved any valid numbers
      if (length(row_values) > 0) {
        # Store the results in a temporary data frame
        temp_df <- data.frame(
          ID = ID,
          year = year,
          B1_RESIDENCIAL = selected_row[1],  # The text in the row
          role_number = paste(row_values, collapse = ", "),  # Join all numbers in the row
          table_index = table_index,  # Record which table the data came from
          stringsAsFactors = FALSE
        )
        
        # Ensure temp_df has the standard column names
        missing_cols <- setdiff(standard_columns, names(temp_df))
        for (col in missing_cols) {
          temp_df[[col]] <- NA  # Add missing columns as NA
        }
        
        # Reorder the columns to match the standard structure
        temp_df <- temp_df[standard_columns]
        
        # Create an output file name
        output_file <- paste0(output_folder, "/", file_name, "_residential.csv")
        
        # Save the data to a CSV file with UTF-8 encoding for Portuguese
        write.csv(temp_df, output_file, row.names = FALSE, fileEncoding = "UTF-8")
        cat("Saved extracted data to:", output_file, "\n")
        
        # Since we found the first valid row, stop processing further tables and exit the loop
        break
      } else {
        cat("No valid number found in the 'B1-RESIDENCIAL' row of table", table_index, "in", pdf_file, "\n")
      }
    } else {
      cat("No fuzzy match for 'B1-RESIDENCIAL' found in table", table_index, "in", pdf_file, "\n")
    }
  }
}

# Loop over all PDF files and apply the extraction function
for (pdf_file in pdf_files) {
  tryCatch({
    extract_first_valid_row_and_save(pdf_file, output_folder)
  }, error = function(e) {
    message("Error processing file ", pdf_file, ": ", e$message)
  })
}


```


#Integrate the results
```{r}

# Set the input folder containing the CSV files
input_folder <- "C:/Users/Ribeiro/Documents/POS-GRADUAÇÃO/POSTDOC/ANEEL_tariff/ANEEL_tariff_residential_csv"

# Set the output CSV file
output_file <- "C:/Users/Ribeiro/Documents/POS-GRADUAÇÃO/POSTDOC/ANEEL_tariff/combined_residential.csv"

# List all CSV files in the input folder
csv_files <- list.files(input_folder, pattern = "\\.csv$", full.names = TRUE)

# Initialize an empty data frame to store combined data
combined_data <- data.frame(ID = character(), year = character(), B1_RESIDENCIAL = character(), stringsAsFactors = FALSE)

# Loop over each CSV file and bind the relevant columns
for (csv_file in csv_files) {
  # Read the CSV file
  df <- read.csv(csv_file, stringsAsFactors = FALSE, fileEncoding = "UTF-8")
  
  # Check if the required columns exist
  if (all(c("ID", "year", "role_number") %in% colnames(df))) {
    # Create a new data frame with the relevant columns, renaming "role_number" to "B1_RESIDENCIAL"
    temp_df <- data.frame(
      ID = df$ID,
      year = df$year,
      B1_RESIDENCIAL = df$role_number, # Map role_number to B1_RESIDENCIAL
      stringsAsFactors = FALSE
    )
    
    # Replace "," with "." and remove spaces after the former comma
    temp_df$B1_RESIDENCIAL <- gsub(",\\s*", ".", temp_df$B1_RESIDENCIAL)
    
    # Bind the data to the combined data frame
    combined_data <- rbind(combined_data, temp_df)
  } else {
    cat("Skipping file:", csv_file, "- Required columns not found\n")
  }
}

# Save the combined data to a CSV file
write.csv(combined_data, output_file, row.names = FALSE, fileEncoding = "UTF-8")

cat("Combined data saved to:", output_file, "\n")



# Ensure the 'year' column is arranged in ascending order
combined_data <- combined_data %>% arrange(year)

# Convert from long to wide format using pivot_wider
wide_df <- pivot_wider(combined_data, names_from = year, values_from = B1_RESIDENCIAL)


```


# Try to improve the analysis from PDF files with OCR - NOT WORK
```{r}

# Download Portuguese language data if not already available
tesseract_download("por")

# Set the TESSDATA_PREFIX environment variable to point to the correct directory
Sys.setenv(TESSDATA_PREFIX = "C:/Users/Ribeiro/AppData/Local/tesseract5/tesseract5/tessdata")  # Adjust this path as necessary

# Set the directory path
pdf_dir <- "C:/Users/Ribeiro/Documents/POS-GRADUAÇÃO/POSTDOC/ANEEL_tariff/ANEEL_tariff_documents"

# List all PDF files in the directory
pdf_files <- list.files(pdf_dir, pattern = "\\.pdf$", full.names = TRUE)

# Filter the files that end with 1997, 1998, or 1999
selected_pdfs <- pdf_files[grepl("1997\\.pdf$|1998\\.pdf$|1999\\.pdf$", pdf_files)]

# Initialize tesseract for OCR (Portuguese language)
tesseract_engine <- tesseract("por")

# Function to process PDFs, apply OCR if necessary, and extract "B1-RESIDENCIAL" values
convert_pdf_to_readable <- function(pdf_path) {
  message("Processing file: ", pdf_path)
  
  # Read PDF using pdftools
  pdf_text_raw <- pdf_text(pdf_path)
  
  # Check if pdf_text_raw extracted any text
  if (length(pdf_text_raw) == 0 || all(nchar(pdf_text_raw) == 0)) {
    # If no text extracted, fallback to OCR
    message("Using OCR on: ", pdf_path)
    pdf_text_raw <- pdf_ocr_text(pdf_path, engine = tesseract_engine)
  }
  
  # Loop over pages of the document
  for (i in seq_along(pdf_text_raw)) {
    text_page <- pdf_text_raw[i]
    
    # Print the text of the current page for troubleshooting
    message("Text of page ", i, ":\n", substr(text_page, 1, 500), "\n")
    
    # Use regular expression to find "B1-RESIDENCIAL" followed by the number
    b1_residencial_value <- str_extract_all(text_page, "B1-RESIDENCIAL\\s*[0-9,.]+")
    
    # Check if we found any matches and print them
    if (length(b1_residencial_value[[1]]) > 0) {
      for (value in b1_residencial_value[[1]]) {
        message("Extracted value from page ", i, ": ", value)
      }
    } else {
      message("No 'B1-RESIDENCIAL' found on page ", i)
    }
  }
}

# Loop through the selected PDFs and apply the extraction process
lapply(selected_pdfs, convert_pdf_to_readable)

```


# ???
```{r}


# Define the PDF directory
pdf_dir <- "C:/Users/Ribeiro/Documents/POS-GRADUAÇÃO/POSTDOC/ANEEL_tariff/ANEEL_tariff_documents"

# List all PDF files in the directory
pdf_files <- list.files(pdf_dir, pattern = "\\.pdf$", full.names = TRUE)

# Filter the files that end with 1997, 1998, or 1999
selected_pdfs <- pdf_files[grepl("1997\\.pdf$|1998\\.pdf$|1999\\.pdf$", pdf_files)]

# Initialize tesseract for OCR in Portuguese
tesseract_engine <- tesseract("por")

# Function to process PDFs, convert pages to images, apply OCR, and extract relevant data
extract_b1_residencial_from_image <- function(pdf_path) {
  message("Processing file: ", pdf_path)
  
  # Convert PDF pages to images
  pdf_images <- image_read_pdf(pdf_path)

  # Initialize a list to store extracted values
  extracted_values <- list()
  
  # Loop through the images and apply OCR
  for (i in seq_along(pdf_images)) {
    image_page <- pdf_images[i]
    
    # Apply OCR to the image
    text_page <- ocr(image_page, engine = tesseract_engine)
    
    # Combine all lines into a single string to simulate lines being on the same row
    combined_text <- gsub("\n", " ", text_page)
    
    # Regular expression to find "B1-RESIDENCIAL" followed by the amount
    pattern <- "B1-RESIDENCIAL\\s*([0-9,.]+)"
    
    # Extract the matches for "B1-RESIDENCIAL" with the amount
    matches <- str_extract_all(combined_text, pattern)
    
    # Append the extracted matches to the list
    if (length(matches[[1]]) > 0) {
      extracted_values <- c(extracted_values, matches[[1]])
    } else {
      message("No 'B1-RESIDENCIAL' found on page ", i)
    }
  }
  
  # Return the extracted values
  return(extracted_values)
}

# Loop through the selected PDFs and extract relevant data from images
all_extracted_data <- lapply(selected_pdfs, extract_b1_residencial_from_image)

# Combine all extracted data into a single vector
final_extracted_data <- unlist(all_extracted_data)

# Display the extracted results
print(final_extracted_data)


```


```{r}

getwd()

setwd("C:/Users/Ribeiro/Documents/POS-GRADUAÇÃO/POSTDOC/ANEEL_tariff/ANEEL_tariff_documents")

bank_statement <- pdftools::pdf_convert("CEA_1997.pdf", dpi = 500)

raw_text <- ocr(bank_statement, engine = tesseract("por"))

print(raw_text)

raw_text_combined<- paste(raw_text,collapse="")

deposits_and_additions<- raw_text_combined %>% 
                         # Need to replace \n
                         str_replace_all('\\n',';') %>%
                         str_extract("(?<=B1-RESIDENCIAL).*(?=B1-RESIDENCIAL)") %>% 
                         str_extract('\\d{2}\\/\\d{2}.*\\.\\d{2}') %>%
                         str_split(';') %>% 
                         as_tibble(.name_repair = "unique") %>% 
                         transmute(consumo = ...1 %>% str_extract("(?<=[A-z] ).*")%>% str_extract('\\d{1,}.*'))

```

